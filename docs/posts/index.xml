<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>All Posts - Bluz Space</title>
        <link>https://bluzioo.github.io/posts/</link>
        <description>All Posts | Bluz Space</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>bluz.mao@outlook.com (maozw)</managingEditor>
            <webMaster>bluz.mao@outlook.com (maozw)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 15 Sep 2021 17:21:09 &#43;0800</lastBuildDate><atom:link href="https://bluzioo.github.io/posts/" rel="self" type="application/rss+xml" /><item>
    <title>loft vcluster初探</title>
    <link>https://bluzioo.github.io/vcluster/</link>
    <pubDate>Wed, 15 Sep 2021 17:21:09 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://bluzioo.github.io/vcluster/</guid>
    <description><![CDATA[概述 Kubernets 原生提供的多租户的方案是通过namespace进行的，这种方式是多个用户共享同一个集群和控制平面，工作负载或者应用。如果CRD资源是cluster-scope，要想让不同用户可以使用不同版本的CRD版本，以namespace就无法做到这等隔离性。
vcluster 是运行在Kubernetes集群上的虚拟集群。vcluster虚拟集群拥有自己的控制平面，不过vcluster没有自己的node节点，pod工作负载交由底层的实际k8s集群进行调度。vcluster 的出现解决 namespace 作为租户隔离方案的一些问题，比如：
  cluster-scope类型的资源在集群里是全局的，并不能通过namespace进行隔离，像istio等组件在同一集群里同时运行不同版本是不可行的
  即使以namesapce隔离，同一集群还是共享这控制平面，如果对控制平面进行配置调整有误的话，将会造成整个集群不可用
  vcluster 与其他隔离方案的一个对比参考如下官网的宣传 vcluster 架构 vcluster本身只包含核心 Kubernetes 组件：API 服务器、控制器管理器和存储后端（如 etcd、sqlite、mysql 等）。为了减少虚拟集群开销，vcluster 构建在k3s 上，这是一个完全有效的、经过认证的、轻量级的 Kubernetes 发行版，它将 Kubernetes 组件编译成单个二进制文件并禁用所有不需要的 Kubernetes 功能，例如 pod 调度程序或某些控制器。
除了 k3s 之外，还有一个 Kubernetes hypervisor 替代了 Kubernetes 调度程序，并在虚拟集群中模拟完全工作的 Kubernetes 设置。该组件同步一些对虚拟和主机集群之间的集群功能至关重要的核心资源：
  Pods：所有在虚拟集群中启动的 Pod 都被重写，然后在宿主机集群中以虚拟集群的命名空间中启动。服务帐户令牌、环境变量、DNS 和其他配置被交换以指向虚拟集群而不是主机集群。在 pod 中，pod 似乎是在虚拟集群中而不是主机集群中启动的。
  Service：所有服务和端点都在主机集群中的虚拟集群的命名空间中重写和创建。虚拟集群和主机集群共享相同的服务集群 IP。这也意味着可以从虚拟集群内部访问主机集群中的服务，而不会造成任何性能损失。
  PersistentVolumeClaims：如果在虚拟集群中创建了持久卷声明，它们将在主机集群中的虚拟集群的命名空间中进行变异和创建。如果它们绑定在主机集群中，则相应的持久卷信息将同步回虚拟集群。
  Configmaps &amp; Secrets：挂载到 pod 的虚拟集群中的 ConfigMaps 或 secrets 将同步到主机集群，所有其他 configmaps 或 secrets 将纯粹保留在虚拟集群中。]]></description>
</item><item>
    <title>Istio 初探</title>
    <link>https://bluzioo.github.io/istio-learning/</link>
    <pubDate>Thu, 26 Aug 2021 11:35:08 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://bluzioo.github.io/istio-learning/</guid>
    <description><![CDATA[概述 回望应用架构的演进，单体 -&gt; SOA -&gt; 微服务，不同的架构在一个企业发展的不同时期都有重要的意义。
在微服务架构中，一个应用程序基于领域驱动设计分解成不同的小服务，服务间的通信使用HTTP Rest、gRPC等协议，为满足服务治理等需求，微服务中间件都会提供不同语言的SDK供服务组件集成。
微服务架构优势在于模块化，扩展性，分布式等，然而在渐渐的落地实践时，也暴露出了一些问题。微服务SDK囊括了服务注册发现，负载均衡，熔断，限流等功能，与业务应用耦合在一个可执行程序，这在SDK需要升级时，本与业务无关的需求，业务应用也需进行发布升级。同时，因服务的异构性，对不同语言的SDK进行维护也是一种煎熬。于是，为了使服务间的通信进一步与业务应用解耦，催生了服务网格（Service Mesh）。
服务网格 Service Mesh 一词最早由开发 Linkerd 的 Buoyant 公司提出，Willian Morgan，Buoyant CEO 对 Service Mesh 的解释：
 Service Mesh 是一个专门处理服务通讯的基础设施层。它的职责是在由云原生应用组成服务的复杂拓扑结构下进行可靠的请求传送。在实践中，它是一组和应用服务部署在一起的轻量级的网络代理，并且对应用服务透明。
 简单地说，假设把服务网格比作是应用程序或者说微服务间的 TCP/IP，负责服务之间的网络调用、限流、熔断和监控。对于编写应用程序来说一般无须关心 TCP/IP 这一层（比如通过 HTTP 协议的 RESTful 应用），同样使用 Service Mesh 也就无须关心服务之间的那些原本通过服务框架实现的事情，比如 Spring Cloud、Netflix OSS 和其他中间件，现在只要交给 Service Mesh 就可以了。
服务网格的需求包括服务发现、负载均衡、故障恢复、度量和监控等。服务网格通常还有更复杂的运维需求，比如 A/B 测试、金丝雀发布、速率限制、访问控制和端到端认证。
目前两款流行的 Service Mesh 开源软件 Istio 和 Linkerd 都可以直接在 Kubernetes 中集成，其中 Linkerd 是最早开源的，已经成为 CNCF 中的项目，而Istio 是由 Google、IBM、Lyft 等共同开源的 Service Mesh 框架，于2017年开源。]]></description>
</item><item>
    <title>Kubernetes裸金属环境负载均衡器-MetalLB</title>
    <link>https://bluzioo.github.io/metallb/</link>
    <pubDate>Tue, 10 Aug 2021 15:38:37 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://bluzioo.github.io/metallb/</guid>
    <description><![CDATA[背景 Kubernetes service对外提供服务的方式有NodePort/ClusterIP/Ingress/LoadBalancer。NodePort以集群内主机绑定随机或自定义端口方式暴露服务，存在单点问题，不会用于生产环境。Ingress基于7层协议，只支持http/https，搭配Ingress Controller再前置LVS-keepalived或LoadBalancer才可对外提供高可用服务。
在公有云的K8S集群通常使用云厂商的LB对外暴露的方式，如AWS的ELB。由于K8S本身不提供LB服务，就需解决自建方式（裸金属环境）的负载均衡器缺失问题。
裸金属环境的负载均衡器开源解决方案：
 MetalLB OpenELB （原PorterLB）  以下对MetalLB进行展开
概念 MetalLB具备两项功能：address allocation和external announcement
Address Allocation MetalLB支持地址分配，前提是配置好IP池。面向内网的集群，即分配好内网IP段，而面向公网的，也可列举可用的公网IP列表。
如果不指定service的spec.loadbalancerIP，metalLB则会从IP池随机分配给LoadBalancer型的service。
External Announcement 分配了的LB IP对外又是怎么被发现及访问到？MetalLB提供了两种模式：Layer 2和BGP。
在Layer 2模式下，可指定几台集群内主机作为响应LB IP的地址发现（ARP for IPv4, NDP for IPv6）。
在BGP模式下，需要上层路由器支持BGP协议，MetalLB与路由器建立BGP session，更新LB IP的路由信息。
部署 环境要求  Kubernetes version &gt; 1.13.0, 集群没有负载均衡器 集群网络兼容MetalLB，兼容清单https://metallb.universe.tf/installation/network-addons/ 空闲的IP或IP段可供MetalLB分配 BGP模式需要支持BGP协议的上层路由器 7946端口在节点间开放  测试环境    组件 描述     系统 centos7   内核 5.13.0-1.el7.elrepo.x86_64   kubernetes v1.19.12   cilium v1.]]></description>
</item></channel>
</rss>
