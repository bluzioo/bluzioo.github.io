<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>kubernetes - Category - Bluz Space</title>
        <link>https://bluzioo.github.io/categories/kubernetes/</link>
        <description>kubernetes - Category - Bluz Space</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>bluz.mao@outlook.com (maozw)</managingEditor>
            <webMaster>bluz.mao@outlook.com (maozw)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sat, 18 Sep 2021 00:18:17 &#43;0800</lastBuildDate><atom:link href="https://bluzioo.github.io/categories/kubernetes/" rel="self" type="application/rss+xml" /><item>
    <title>Telepresence 容器化微服务本地开发</title>
    <link>https://bluzioo.github.io/telepresence/</link>
    <pubDate>Sat, 18 Sep 2021 00:18:17 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://bluzioo.github.io/telepresence/</guid>
    <description><![CDATA[概述 在对微服务进行容器化部署至 Kubernetes 这个过程属于持续交付阶段，已经通过 Argo CD 等交付工具解决。然而，如果我们要在本地开发调试一个服务 A，但服务 A 可能依赖服务B、C，而服务 B 又有一层依赖 D，我们就需要在本地把服务 B、C、D 都搭建起来才能调试服务 A。这显然是一个很痛苦的过程。
Telepresence 是一个 CNCF 基金会下的项目。它的工作原理是在本地和 Kubernetes 集群中搭建一个透明的双向代理，这使得我们可以在本地用熟悉的 IDE 和调试工具来运行一个微服务，同时该服务还可以无缝的与 Kubernetes 集群中的其他服务进行交互，好像它就运行在这个集群中一样。
Telepresence 实践 安装 在本地环境安装，参考 https://www.telepresence.io/docs/latest/install/
使用场景 以 sage 服务为例，在 ARGO CD 配置 sage 服务部署至 Kubernetes 开发测试环境，查看 sage 服务是否运行。
1 2 3  [root@k8s-master01 ~]# kubectl get pod -n wjyl-dev NAME READY STATUS RESTARTS AGE sage-aaa5686745-xxft9 2/2 Running 1 7h38m   此时 sage pod 有两个 container 一个是 sage 服务，另一个是 istio-proxy。请求下测试接口，显示的数据是 &ldquo;from k8s&rdquo;]]></description>
</item><item>
    <title>Loft VCluster 虚拟集群</title>
    <link>https://bluzioo.github.io/vcluster/</link>
    <pubDate>Wed, 15 Sep 2021 17:21:09 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://bluzioo.github.io/vcluster/</guid>
    <description><![CDATA[概述 Kubernets 原生提供的多租户的方案是通过namespace进行的，这种方式是多个用户共享同一个集群和控制平面，工作负载或者应用。如果CRD资源是cluster-scope，要想让不同用户可以使用不同版本的CRD版本，以namespace就无法做到这等隔离性。
vcluster 是运行在Kubernetes集群上的虚拟集群。vcluster虚拟集群拥有自己的控制平面，不过vcluster没有自己的node节点，pod工作负载交由底层的实际k8s集群进行调度。vcluster 的出现解决 namespace 作为租户隔离方案的一些问题，比如：
  cluster-scope类型的资源在集群里是全局的，并不能通过namespace进行隔离，像istio等组件在同一集群里同时运行不同版本是不可行的
  即使以namesapce隔离，同一集群还是共享这控制平面，如果对控制平面进行配置调整有误的话，将会造成整个集群不可用
  vcluster 与其他隔离方案的一个对比参考如下官网的宣传 vcluster 架构 vcluster本身只包含核心 Kubernetes 组件：API 服务器、控制器管理器和存储后端（如 etcd、sqlite、mysql 等）。为了减少虚拟集群开销，vcluster 构建在k3s 上，这是一个完全有效的、经过认证的、轻量级的 Kubernetes 发行版，它将 Kubernetes 组件编译成单个二进制文件并禁用所有不需要的 Kubernetes 功能，例如 pod 调度程序或某些控制器。
除了 k3s 之外，还有一个 Kubernetes hypervisor 替代了 Kubernetes 调度程序，并在虚拟集群中模拟完全工作的 Kubernetes 设置。该组件同步一些对虚拟和主机集群之间的集群功能至关重要的核心资源：
  Pods：所有在虚拟集群中启动的 Pod 都被重写，然后在宿主机集群中以虚拟集群的命名空间中启动。服务帐户令牌、环境变量、DNS 和其他配置被交换以指向虚拟集群而不是主机集群。在 pod 中，pod 似乎是在虚拟集群中而不是主机集群中启动的。
  Service：所有服务和端点都在主机集群中的虚拟集群的命名空间中重写和创建。虚拟集群和主机集群共享相同的服务集群 IP。这也意味着可以从虚拟集群内部访问主机集群中的服务，而不会造成任何性能损失。
  PersistentVolumeClaims：如果在虚拟集群中创建了持久卷声明，它们将在主机集群中的虚拟集群的命名空间中进行变异和创建。如果它们绑定在主机集群中，则相应的持久卷信息将同步回虚拟集群。
  Configmaps &amp; Secrets：挂载到 pod 的虚拟集群中的 ConfigMaps 或 secrets 将同步到主机集群，所有其他 configmaps 或 secrets 将纯粹保留在虚拟集群中。]]></description>
</item><item>
    <title>Kubernetes 裸金属环境负载均衡器 - MetalLB</title>
    <link>https://bluzioo.github.io/metallb/</link>
    <pubDate>Tue, 10 Aug 2021 15:38:37 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://bluzioo.github.io/metallb/</guid>
    <description><![CDATA[背景 Kubernetes service对外提供服务的方式有NodePort/ClusterIP/Ingress/LoadBalancer。NodePort以集群内主机绑定随机或自定义端口方式暴露服务，存在单点问题，不会用于生产环境。Ingress基于7层协议，只支持http/https，搭配Ingress Controller再前置LVS-keepalived或LoadBalancer才可对外提供高可用服务。
在公有云的K8S集群通常使用云厂商的LB对外暴露的方式，如AWS的ELB。由于K8S本身不提供LB服务，就需解决自建方式（裸金属环境）的负载均衡器缺失问题。
裸金属环境的负载均衡器开源解决方案：
 MetalLB OpenELB （原PorterLB）  以下对MetalLB进行展开
概念 MetalLB具备两项功能：address allocation和external announcement
Address Allocation MetalLB支持地址分配，前提是配置好IP池。面向内网的集群，即分配好内网IP段，而面向公网的，也可列举可用的公网IP列表。
如果不指定service的spec.loadbalancerIP，metalLB则会从IP池随机分配给LoadBalancer型的service。
External Announcement 分配了的LB IP对外又是怎么被发现及访问到？MetalLB提供了两种模式：Layer 2和BGP。
在Layer 2模式下，可指定几台集群内主机作为响应LB IP的地址发现（ARP for IPv4, NDP for IPv6）。
在BGP模式下，需要上层路由器支持BGP协议，MetalLB与路由器建立BGP session，更新LB IP的路由信息。
部署 环境要求  Kubernetes version &gt; 1.13.0, 集群没有负载均衡器 集群网络兼容MetalLB，兼容清单https://metallb.universe.tf/installation/network-addons/ 空闲的IP或IP段可供MetalLB分配 BGP模式需要支持BGP协议的上层路由器 7946端口在节点间开放  测试环境    组件 描述     系统 centos7   内核 5.13.0-1.el7.elrepo.x86_64   kubernetes v1.19.12   cilium v1.]]></description>
</item></channel>
</rss>
