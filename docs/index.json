[{"categories":["kubernetes","devops"],"content":"telepresence 本地开发调试工具","date":"2021-09-18","objectID":"/telepresence/","tags":["kubernetes","devops"],"title":"Telepresence","uri":"/telepresence/"},{"categories":["kubernetes","devops"],"content":"概述 在对微服务进行容器化部署至 Kubernetes 这个过程属于持续交付阶段，已经通过 Argo CD 等交付工具解决。然而，如果我们要在本地开发调试一个服务 A，但服务 A 可能依赖服务B、C，而服务 B 又有一层依赖 D，我们就需要在本地把服务 B、C、D 都搭建起来才能调试服务 A。这显然是一个很痛苦的过程。 Telepresence 是一个 CNCF 基金会下的项目。它的工作原理是在本地和 Kubernetes 集群中搭建一个透明的双向代理，这使得我们可以在本地用熟悉的 IDE 和调试工具来运行一个微服务，同时该服务还可以无缝的与 Kubernetes 集群中的其他服务进行交互，好像它就运行在这个集群中一样。 ","date":"2021-09-18","objectID":"/telepresence/:1:0","tags":["kubernetes","devops"],"title":"Telepresence","uri":"/telepresence/"},{"categories":["kubernetes","devops"],"content":"Telepresence 实践 ","date":"2021-09-18","objectID":"/telepresence/:2:0","tags":["kubernetes","devops"],"title":"Telepresence","uri":"/telepresence/"},{"categories":["kubernetes","devops"],"content":"安装 在本地环境安装，参考 https://www.telepresence.io/docs/latest/install/ ","date":"2021-09-18","objectID":"/telepresence/:2:1","tags":["kubernetes","devops"],"title":"Telepresence","uri":"/telepresence/"},{"categories":["kubernetes","devops"],"content":"使用场景 以 sage 服务为例，在 ARGO CD 配置 sage 服务部署至 Kubernetes 开发测试环境，查看 sage 服务是否运行。 [root@k8s-master01 ~]# kubectl get pod -n wjyl-dev NAME READY STATUS RESTARTS AGE sage-aaa5686745-xxft9 2/2 Running 1 7h38m 此时 sage pod 有两个 container 一个是 sage 服务，另一个是 istio-proxy。请求下测试接口，显示的数据是 “from k8s” [root@k8s-master01 ~]# curl -v -H \"Authorization: Bearer eyJhbGciOiJIUzUxMiJ9.eyJpc3MiOiJjYWNhbyIsInN1YiI6ImF5Zm9vZCIsImF1ZCI6IndlYiIsImlhdCI6MTYzMTUxOTAzNiwiZXhwIjoxNjMxNTYyMjM2LCJvcmdfdW5pdCI6InRlbmFudCJ9.OEGQ5LIzfAFXyaSUuU4ZS1koaJ0ufAJK-j97fjhqpRf5j5YqCugbYW37Je-I6dfPdMQQwlXK-WlpU3MCxmM-lg\" sage.dev.netfuse.cn/v1/usage/test {\"code\":\"0\",\"msg\":\"\",\"date\":1632237007295,\"data\": \"from k8s\"} 在本地开发环境，运行 telepresence connect 连接至 kubernetes 开发测试环境。 $ telepresence connect Launching Telepresence Root Daemon Need root privileges to run: /usr/local/bin/telepresence daemon-foreground /Users/bluz/Library/Logs/telepresence '/Users/bluz/Library/Application Support/telepresence' '' Password: Launching Telepresence User Daemon Connected to context dev01-context (https://192.168.199.77:6443) 此时 Kubernetes 集群会部署 telepresence 流量管理的服务。 [root@k8s-master01 ~]# kubectl get pod -n ambassador NAME READY STATUS RESTARTS AGE traffic-manager-85d645d956-6vswr 1/1 Running 1 3d23h 使用 telepresence 拦截集群中 sage 服务的流量至本地开发环境。 $ telepresence intercept sage --port 10519:50100 -n wjyl-dev Using Deployment sage intercepted Intercept name : sage-wjyl-dev State : ACTIVE Workload kind : Deployment Destination : 127.0.0.1:10519 Service Port Identifier: 50100 Volume Mount Error : sshfs is not installed on your local machine Intercepting : all TCP connections 修改本地测试接口返回的数据为 “from local”，以 debug 模式启动 sage 服务，监听端口为 10519。再次请求测试接口，显示的数据即是本地修改 “from local”。在 IDE 设置断点即可调试。 [root@k8s-master01 ~]# curl -H \"Authorization: Bearer eyJhbGciOiJIUzUxMiJ9.eyJpc3MiOiJjYWNhbyIsInN1YiI6ImF5Zm9vZCIsImF1ZCI6IndlYiIsImlhdCI6MTYzMTUxOTAzNiwiZXhwIjoxNjMxNTYyMjM2LCJvcmdfdW5pdCI6InRlbmFudCJ9.OEGQ5LIzfAFXyaSUuU4ZS1koaJ0ufAJK-j97fjhqpRf5j5YqCugbYW37Je-I6dfPdMQQwlXK-WlpU3MCxmM-lg\" sage.dev.netfuse.cn/v1/usage/test {\"code\":\"0\",\"msg\":\"\",\"date\":1632237007295,\"data\": \"from local\"} ","date":"2021-09-18","objectID":"/telepresence/:2:2","tags":["kubernetes","devops"],"title":"Telepresence","uri":"/telepresence/"},{"categories":["kubernetes","devops"],"content":"参考 https://www.telepresence.io/docs/latest/quick-start/ https://jimmysong.io/blog/how-to-debug-microservices-in-kubernetes-with-proxy-sidecar-or-service-mesh/ https://zhuanlan.zhihu.com/p/106051607 ","date":"2021-09-18","objectID":"/telepresence/:3:0","tags":["kubernetes","devops"],"title":"Telepresence","uri":"/telepresence/"},{"categories":["kubernetes"],"content":"Kubernetes虚拟集群","date":"2021-09-15","objectID":"/vcluster/","tags":["kubernetes","k3s"],"title":"loft vcluster 初探","uri":"/vcluster/"},{"categories":["kubernetes"],"content":"概述 Kubernets 原生提供的多租户的方案是通过namespace进行的，这种方式是多个用户共享同一个集群和控制平面，工作负载或者应用。如果CRD资源是cluster-scope，要想让不同用户可以使用不同版本的CRD版本，以namespace就无法做到这等隔离性。 vcluster 是运行在Kubernetes集群上的虚拟集群。vcluster虚拟集群拥有自己的控制平面，不过vcluster没有自己的node节点，pod工作负载交由底层的实际k8s集群进行调度。vcluster 的出现解决 namespace 作为租户隔离方案的一些问题，比如： cluster-scope类型的资源在集群里是全局的，并不能通过namespace进行隔离，像istio等组件在同一集群里同时运行不同版本是不可行的 即使以namesapce隔离，同一集群还是共享这控制平面，如果对控制平面进行配置调整有误的话，将会造成整个集群不可用 vcluster 与其他隔离方案的一个对比参考如下官网的宣传 ","date":"2021-09-15","objectID":"/vcluster/:1:0","tags":["kubernetes","k3s"],"title":"loft vcluster 初探","uri":"/vcluster/"},{"categories":["kubernetes"],"content":"vcluster 架构 vcluster本身只包含核心 Kubernetes 组件：API 服务器、控制器管理器和存储后端（如 etcd、sqlite、mysql 等）。为了减少虚拟集群开销，vcluster 构建在k3s 上，这是一个完全有效的、经过认证的、轻量级的 Kubernetes 发行版，它将 Kubernetes 组件编译成单个二进制文件并禁用所有不需要的 Kubernetes 功能，例如 pod 调度程序或某些控制器。 除了 k3s 之外，还有一个 Kubernetes hypervisor 替代了 Kubernetes 调度程序，并在虚拟集群中模拟完全工作的 Kubernetes 设置。该组件同步一些对虚拟和主机集群之间的集群功能至关重要的核心资源： Pods：所有在虚拟集群中启动的 Pod 都被重写，然后在宿主机集群中以虚拟集群的命名空间中启动。服务帐户令牌、环境变量、DNS 和其他配置被交换以指向虚拟集群而不是主机集群。在 pod 中，pod 似乎是在虚拟集群中而不是主机集群中启动的。 Service：所有服务和端点都在主机集群中的虚拟集群的命名空间中重写和创建。虚拟集群和主机集群共享相同的服务集群 IP。这也意味着可以从虚拟集群内部访问主机集群中的服务，而不会造成任何性能损失。 PersistentVolumeClaims：如果在虚拟集群中创建了持久卷声明，它们将在主机集群中的虚拟集群的命名空间中进行变异和创建。如果它们绑定在主机集群中，则相应的持久卷信息将同步回虚拟集群。 Configmaps \u0026 Secrets：挂载到 pod 的虚拟集群中的 ConfigMaps 或 secrets 将同步到主机集群，所有其他 configmaps 或 secrets 将纯粹保留在虚拟集群中。 其他资源：部署、状态集、CRD、服务帐户等不同步到主机集群，纯粹存在于虚拟集群中。 除了虚拟和主机集群资源的同步之外，hypervisor 还代理某些 Kubernetes API 请求到主机集群，例如 pod 端口转发或容器命令执行。它本质上充当虚拟集群的反向代理。 ","date":"2021-09-15","objectID":"/vcluster/:2:0","tags":["kubernetes","k3s"],"title":"loft vcluster 初探","uri":"/vcluster/"},{"categories":["kubernetes"],"content":"vcluster 实践 ","date":"2021-09-15","objectID":"/vcluster/:3:0","tags":["kubernetes","k3s"],"title":"loft vcluster 初探","uri":"/vcluster/"},{"categories":["kubernetes"],"content":"部署 下载vcluster CLI curl -s -L \"https://github.com/loft-sh/vcluster/releases/latest\" | sed -nE 's!.*\"([^\"]*vcluster-darwin-amd64)\".*!https://github.com\\1!p' | xargs -n 1 curl -L -o vcluster \u0026\u0026 chmod +x vcluster; sudo mv vcluster /usr/local/bin; 创建虚拟集群 vcluster create vcluster-1 -n host-namespace-1 vcluster create vcluster-1 -n host-namespace-1 --expose vcluster 另外也可以通过helm，kubectl创建虚拟集群 连接虚拟集群 vcluster connect vcluster-1 -n host-namespace-1 在当前目录会生产一份kubeconfig.yaml 使用虚拟集群 # 另启一个终端，进入上一步的目录 export KUBECONFIG=./kubeconfig.yaml # 查看虚拟集群的namespace列表 kubectl get namespace # 查看虚拟集群的kube-system下的pod列表 kubectl get pods -n kube-system 验证虚拟集群 kubectl create namespace demo-nginx kubectl create deployment nginx-deployment -n demo-nginx --image=nginx kubectl get pods -n demo-nginx 而在主集群发生什么？ 查看主机群的命名空间， 发现存在先前创建的虚拟集群存在于主集群命名空间 kubectl get namespaces NAME STATUS AGE default Active 11d host-namespace-1 Active 9m17s kube-node-lease Active 11d kube-public Active 11d kube-system Active 11d 查看在host-namespace-1下的deployment，并不存在nginx-deployment，表明vcluster不会同步deployment资源 kubectl get deployments -n host-namespace-1 No resources found in host-namespace-1 namespace. 查看在host-namespace-1下的pods，存在一个名为nginx-deployment-84cd76b964-mnvzz-x-demo-nginx-x-vcluster-1的pod kubectl get pods -n host-namespace-1 NAME READY STATUS RESTARTS AGE coredns-66c464876b-p275l-x-kube-system-x-vcluster-1 1/1 Running 0 14m nginx-deployment-84cd76b964-mnvzz-x-demo-nginx-x-vcluster-1 1/1 Running 0 10m vcluster-1-0 2/2 Running 0 14m ","date":"2021-09-15","objectID":"/vcluster/:3:1","tags":["kubernetes","k3s"],"title":"loft vcluster 初探","uri":"/vcluster/"},{"categories":["kubernetes"],"content":"小结 vcluster的优势如下： 轻量级和低开销：它是基于 K3S，捆绑在一个 Pod 中，具有超低的资源消耗。 无性能损耗：所有的 Pod 被调度在底层主机集群中，因此它们在运行时不会受到任何性能影响。 减少主机集群的开销：将大型多租户集群分割成较小的 vcluster ，以减少复杂性并提高可扩展性。 灵活而简单的配置：可以通过 vcluster CLI、Helm、Kubectl、Argo 等任何工具来创建（它基本上只是一个 StatefulSet）。 不需要管理权限：如果你能将 Web 应用部署到 Kubernetes 命名空间，你也能部署 Vcluster。 单一命名空间封装：每个 vcluster 及其所有的工作负载都在底层主机集群的单一命名空间内。 易于清理：删除主机命名空间，Vcluster 及其所有工作负载将立即被清除。 ","date":"2021-09-15","objectID":"/vcluster/:4:0","tags":["kubernetes","k3s"],"title":"loft vcluster 初探","uri":"/vcluster/"},{"categories":["kubernetes"],"content":"参考 https://www.vcluster.com/docs https://liangyuanpeng.com/post/k8s-multi-tenancy-guide/ ","date":"2021-09-15","objectID":"/vcluster/:5:0","tags":["kubernetes","k3s"],"title":"loft vcluster 初探","uri":"/vcluster/"},{"categories":["istio"],"content":"Istio服务网格","date":"2021-08-26","objectID":"/istio-learning/","tags":["istio"],"title":"Istio 初探","uri":"/istio-learning/"},{"categories":["istio"],"content":"概述 回望应用架构的演进，单体 -\u003e SOA -\u003e 微服务，不同的架构在一个企业发展的不同时期都有重要的意义。 在微服务架构中，一个应用程序基于领域驱动设计分解成不同的小服务，服务间的通信使用HTTP Rest、gRPC等协议，为满足服务治理等需求，微服务中间件都会提供不同语言的SDK供服务组件集成。 微服务架构优势在于模块化，扩展性，分布式等，然而在渐渐的落地实践时，也暴露出了一些问题。微服务SDK囊括了服务注册发现，负载均衡，熔断，限流等功能，与业务应用耦合在一个可执行程序，这在SDK需要升级时，本与业务无关的需求，业务应用也需进行发布升级。同时，因服务的异构性，对不同语言的SDK进行维护也是一种煎熬。于是，为了使服务间的通信进一步与业务应用解耦，催生了服务网格（Service Mesh）。 ","date":"2021-08-26","objectID":"/istio-learning/:1:0","tags":["istio"],"title":"Istio 初探","uri":"/istio-learning/"},{"categories":["istio"],"content":"服务网格 Service Mesh 一词最早由开发 Linkerd 的 Buoyant 公司提出，Willian Morgan，Buoyant CEO 对 Service Mesh 的解释： Service Mesh 是一个专门处理服务通讯的基础设施层。它的职责是在由云原生应用组成服务的复杂拓扑结构下进行可靠的请求传送。在实践中，它是一组和应用服务部署在一起的轻量级的网络代理，并且对应用服务透明。 简单地说，假设把服务网格比作是应用程序或者说微服务间的 TCP/IP，负责服务之间的网络调用、限流、熔断和监控。对于编写应用程序来说一般无须关心 TCP/IP 这一层（比如通过 HTTP 协议的 RESTful 应用），同样使用 Service Mesh 也就无须关心服务之间的那些原本通过服务框架实现的事情，比如 Spring Cloud、Netflix OSS 和其他中间件，现在只要交给 Service Mesh 就可以了。 服务网格的需求包括服务发现、负载均衡、故障恢复、度量和监控等。服务网格通常还有更复杂的运维需求，比如 A/B 测试、金丝雀发布、速率限制、访问控制和端到端认证。 目前两款流行的 Service Mesh 开源软件 Istio 和 Linkerd 都可以直接在 Kubernetes 中集成，其中 Linkerd 是最早开源的，已经成为 CNCF 中的项目，而Istio 是由 Google、IBM、Lyft 等共同开源的 Service Mesh 框架，于2017年开源。 ","date":"2021-08-26","objectID":"/istio-learning/:2:0","tags":["istio"],"title":"Istio 初探","uri":"/istio-learning/"},{"categories":["istio"],"content":"Istio 简介 Istio 可以轻松地创建一个已经部署了服务的网络，而服务的代码只需很少更改甚至无需更改。通过在整个环境中部署一个特殊的 sidecar 代理为服务添加 Istio 的支持，而代理会拦截微服务之间的所有网络通信，然后使用其控制平面的功能来配置和管理 Istio，这包括： 为 HTTP、gRPC、WebSocket 和 TCP 流量自动负载均衡。 通过丰富的路由规则、重试、故障转移和故障注入对流量行为进行细粒度控制。 可插拔的策略层和配置 API，支持访问控制、速率限制和配额。 集群内（包括集群的入口和出口）所有流量的自动化度量、日志记录和追踪。 在具有强大的基于身份验证和授权的集群中实现安全的服务间通信。 Istio 为可扩展性而设计，可以满足不同的部署需求。 ","date":"2021-08-26","objectID":"/istio-learning/:3:0","tags":["istio"],"title":"Istio 初探","uri":"/istio-learning/"},{"categories":["istio"],"content":"Istio 架构 Istio 服务网格从逻辑上分为数据平面和控制平面。 数据平面由一组智能代理（Envoy）组成，被部署为 Sidecar。这些代理负责协调和控制微服务之间的所有网络通信。它们还收集和报告所有网格流量的遥测数据。 控制平面管理并配置代理来进行流量路由。 下图展示了组成每个平面的不同组件： ","date":"2021-08-26","objectID":"/istio-learning/:4:0","tags":["istio"],"title":"Istio 初探","uri":"/istio-learning/"},{"categories":["istio"],"content":"数据平面 Istio 在数据平面使用 Envoy 代理的扩展版本。Envoy 是用 C++ 开发的高性能代理，用于协调服务网格中所有服务的入站和出站流量。Envoy 代理是唯一与数据平面流量交互的 Istio 组件。 Envoy 架构 Envoy 接收到请求后，会先走 FilterChain，通过各种 L3/L4/L7 Filter 对请求进行微处理，然后再路由到指定的集群，并通过负载均衡获取一个目标地址，最后再转发出去。 其中每一个环节可以静态配置，也可以动态服务发现，也就是所谓的 xDS。 xDS-Api Envoy xDS 为 Istio 控制平面与数据平面通信的基本协议，只要代理支持该协议表达形式就可以创建自己 Sidecar 来替换 Envoy。 Envoy中，提供的xDS-API列举如下： SDS/EDS(Service/Endpoint(v2) Discovery Service): 节点发现服务，针对的是那些提供服务的节点(可能是一个pod 或者其他),让节点可以聚合成服务的方式提供给调用方。V2版本API中，Service 升级为Endpoint. CDS(Cluster Discovery Service):集群发现服务，集群指的是Envoy接管的集群。Istio可以利用这个接口创建虚拟机集群，例如一个应用可以划分不同版本的部署结构。 RDS(Route Discovery Service): 路由规则发现服务，路由规则的作用就是动态转发，基于此可以实现请求漂移，蓝绿发布等。 LDS(Listener Discovery Service): 监听器发现服务，监听器主要作用于Envoy的链接状态，如链接总数，活动的连接数等。 ","date":"2021-08-26","objectID":"/istio-learning/:4:1","tags":["istio"],"title":"Istio 初探","uri":"/istio-learning/"},{"categories":["istio"],"content":"控制平面 Istiod 提供服务发现、配置和证书管理。 Istiod 将控制流量行为的高级路由规则转换为 Envoy 特定的配置，并在运行时将其传播给 Sidecar。Pilot 提取了特定平台的服务发现机制，并将其综合为一种标准格式，任何符合 Envoy API 的 Sidecar 都可以使用。 Pilot 为了实现对不同服务注册中心 （Kubernetes、consul） 的支持，Pilot 需要对不同的输入来源的数据有一个统一的存储格式，也就是抽象模型。 Pilot 的实现是基于平台适配器（Platform adapters） 的，借助平台适配器 Pilot 可以实现服务注册中心数据到抽象模型之间的数据转换 Pilot 使用了一套起源于 Envoy 项目的标准数据面 API 来将服务信息和流量规则下发到数据面的 sidecar 中。通过采用该标准 API， Istio 将控制面和数据面进行了解耦，为多种数据平面 sidecar 实现提供了可能性。 ","date":"2021-08-26","objectID":"/istio-learning/:4:2","tags":["istio"],"title":"Istio 初探","uri":"/istio-learning/"},{"categories":["istio"],"content":"Istio 实战 ","date":"2021-08-26","objectID":"/istio-learning/:5:0","tags":["istio"],"title":"Istio 初探","uri":"/istio-learning/"},{"categories":["istio"],"content":"部署 ","date":"2021-08-26","objectID":"/istio-learning/:5:1","tags":["istio"],"title":"Istio 初探","uri":"/istio-learning/"},{"categories":["istio"],"content":"流量管理 apiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:sagenamespace:algospec:host:sage # The name of a service from the service registry subsets:- name:v1labels:app.kubernetes.io/name:sagetrafficPolicy:# Traffic policies to apply (load balancing policy, connection pool sizes, outlier detection)tls:mode:ISTIO_MUTUALconnectionPool:http:http1MaxPendingRequests:1maxRequestsPerConnection:1tcp:maxConnections:1outlierDetection:baseEjectionTime:180.000sconsecutiveErrors:1interval:1.000smaxEjectionPercent:100---apiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:sagenamespace:algospec:hosts:- sagehttp:- route:- destination:host:sagesubset:v1- route:# 默认路由，针对没有匹配到的路由规则- destination:host:sagesubset:v1--- ","date":"2021-08-26","objectID":"/istio-learning/:5:2","tags":["istio"],"title":"Istio 初探","uri":"/istio-learning/"},{"categories":["istio"],"content":"追踪 spec:components:base:enabled:truecni:enabled:falseegressGateways:- enabled:falsename:istio-egressgatewayingressGateways:- enabled:truename:istio-ingressgateway- enabled:truename:dev-netfuse-gatewaynamespace:dev-netfuseistiodRemote:enabled:falsepilot:enabled:truek8s:env:- name:PILOT_TRACE_SAMPLINGvalue:\"100\" ","date":"2021-08-26","objectID":"/istio-learning/:5:3","tags":["istio"],"title":"Istio 初探","uri":"/istio-learning/"},{"categories":["istio"],"content":"安全 ","date":"2021-08-26","objectID":"/istio-learning/:5:4","tags":["istio"],"title":"Istio 初探","uri":"/istio-learning/"},{"categories":["istio"],"content":"JWT Auth apiVersion:security.istio.io/v1beta1kind:RequestAuthenticationmetadata:name:jwt-examplenamespace:algospec:jwtRules:- issuer:cacaojwks:\"{ \\\"keys\\\":[{ \\n \\\"kty\\\" : \\\"oct\\\",\\n \\\"kid\\\" : \\\"0afee142-a0af-4410-abcc-9f2d44ff45b5\\\",\\n \\ \\\"alg\\\" : \\\"HS512\\\",\\n \\\"k\\\" : \\\"d2p5bEA4ODg4\\\"\\n}]}\\n\"selector:matchLabels:app.kubernetes.io/name:sage apiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:name:require-jwtnamespace:algospec:action:ALLOWrules:- from:- source:requestPrincipals:- '*'selector:matchLabels:app.kubernetes.io/name:sage ","date":"2021-08-26","objectID":"/istio-learning/:5:5","tags":["istio"],"title":"Istio 初探","uri":"/istio-learning/"},{"categories":["istio"],"content":"场景应用 ","date":"2021-08-26","objectID":"/istio-learning/:6:0","tags":["istio"],"title":"Istio 初探","uri":"/istio-learning/"},{"categories":["istio"],"content":"金丝雀 ","date":"2021-08-26","objectID":"/istio-learning/:6:1","tags":["istio"],"title":"Istio 初探","uri":"/istio-learning/"},{"categories":["istio"],"content":"故障注入 ","date":"2021-08-26","objectID":"/istio-learning/:6:2","tags":["istio"],"title":"Istio 初探","uri":"/istio-learning/"},{"categories":["istio"],"content":"错误诊断 ","date":"2021-08-26","objectID":"/istio-learning/:6:3","tags":["istio"],"title":"Istio 初探","uri":"/istio-learning/"},{"categories":["istio"],"content":"参考 https://istio.io/latest/docs/ https://www.envoyproxy.io/docs/envoy/latest/ https://jimmysong.io/ https://www.servicemesher.com/ https://zhuanlan.zhihu.com/p/369068128 ","date":"2021-08-26","objectID":"/istio-learning/:7:0","tags":["istio"],"title":"Istio 初探","uri":"/istio-learning/"},{"categories":["kubernetes"],"content":"MetalLB是一款使用标准路由协议的K8S负载均衡器","date":"2021-08-10","objectID":"/metallb/","tags":["kubernetes","loadbalancer"],"title":"Kubernetes 裸金属环境负载均衡器 - MetalLB","uri":"/metallb/"},{"categories":["kubernetes"],"content":"背景 Kubernetes service对外提供服务的方式有NodePort/ClusterIP/Ingress/LoadBalancer。NodePort以集群内主机绑定随机或自定义端口方式暴露服务，存在单点问题，不会用于生产环境。Ingress基于7层协议，\u0008只支持http/https，搭配Ingress Controller再前置LVS-keepalived或LoadBalancer才可对外提供高可用服务。 在公有云的K8S集群通常使用云厂商的LB对外暴露的方式，如AWS的ELB。由于K8S本身不提供LB服务，就需解决自建方式（裸金属环境）的负载均衡器缺失问题。 裸金属环境的负载均衡器开源解决方案： MetalLB OpenELB （原PorterLB） 以下对MetalLB进行展开 ","date":"2021-08-10","objectID":"/metallb/:1:0","tags":["kubernetes","loadbalancer"],"title":"Kubernetes 裸金属环境负载均衡器 - MetalLB","uri":"/metallb/"},{"categories":["kubernetes"],"content":"概念 MetalLB具备两项功能：address allocation和external announcement ","date":"2021-08-10","objectID":"/metallb/:2:0","tags":["kubernetes","loadbalancer"],"title":"Kubernetes 裸金属环境负载均衡器 - MetalLB","uri":"/metallb/"},{"categories":["kubernetes"],"content":"Address Allocation MetalLB支持地址分配，前提是配置好IP池。面向内网的集群，即分配好内网IP段，而面向公网的，也可列举可用的公网IP列表。 如果不指定service的spec.loadbalancerIP，\u0008metalLB则会从IP池随机分配给LoadBalancer型的service。 ","date":"2021-08-10","objectID":"/metallb/:2:1","tags":["kubernetes","loadbalancer"],"title":"Kubernetes 裸金属环境负载均衡器 - MetalLB","uri":"/metallb/"},{"categories":["kubernetes"],"content":"External Announcement 分配了的LB IP对外又是怎么被发现及访问到？MetalLB提供了两种模式：Layer 2和BGP。 在Layer 2模式下，可指定几台集群内主机作为响应LB IP的地址发现（ARP for IPv4, NDP for IPv6）。 在BGP模式下，需要上层路由器支持BGP协议，MetalLB与路由器建立BGP session，更新LB IP的路由信息。 ","date":"2021-08-10","objectID":"/metallb/:2:2","tags":["kubernetes","loadbalancer"],"title":"Kubernetes 裸金属环境负载均衡器 - MetalLB","uri":"/metallb/"},{"categories":["kubernetes"],"content":"部署 ","date":"2021-08-10","objectID":"/metallb/:3:0","tags":["kubernetes","loadbalancer"],"title":"Kubernetes 裸金属环境负载均衡器 - MetalLB","uri":"/metallb/"},{"categories":["kubernetes"],"content":"环境要求 Kubernetes version \u003e 1.13.0, 集群没有负载均衡器 集群网络兼容MetalLB，兼容清单https://metallb.universe.tf/installation/network-addons/ 空闲的IP或IP段可供MetalLB分配 BGP模式需要支持BGP协议的上层路由器 7946端口在节点间开放 ","date":"2021-08-10","objectID":"/metallb/:3:1","tags":["kubernetes","loadbalancer"],"title":"Kubernetes 裸金属环境负载均衡器 - MetalLB","uri":"/metallb/"},{"categories":["kubernetes"],"content":"测试环境 组件 描述 系统 centos7 内核 5.13.0-1.el7.elrepo.x86_64 kubernetes v1.19.12 cilium v1.9.3 metallb v0.10.2 master ip 192.168.199.77 LB ip pool 192.168.199.223-224 ","date":"2021-08-10","objectID":"/metallb/:3:2","tags":["kubernetes","loadbalancer"],"title":"Kubernetes 裸金属环境负载均衡器 - MetalLB","uri":"/metallb/"},{"categories":["kubernetes"],"content":"准备 如果k8s集群里使用的是kube-proxy，k8s版本\u003ev1.14.2，需要开启strict ARP，而使用了kube-router作为service proxy，默认已开启。 kubectl edit configmap -n kube-system kube-proxy # 编辑设置 apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: \"ipvs\" ipvs: strictARP: true ","date":"2021-08-10","objectID":"/metallb/:3:3","tags":["kubernetes","loadbalancer"],"title":"Kubernetes 裸金属环境负载均衡器 - MetalLB","uri":"/metallb/"},{"categories":["kubernetes"],"content":"安装 以Manifest安装 kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.10.2/manifests/namespace.yaml kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.10.2/manifests/metallb.yaml 安装后，metallb的资源对象隔离在名为metallb-system的namespace，这时metallb还未能工作，LoadBalancerIP未分配 [root@k8s-master01 ~]# k get svc istio-ingressgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.107.221.79 \u003cpending\u003e 15021:31462/TCP,80:32243/TCP,443:31087/TCP,8081:30765/TCP 36d 除了使用manifests安装，社区还提供helm和kustomize方式安装，详见https://metallb.universe.tf/installation 配置Layer 2模式 开启layer2模式，需配置一份configMap，添加至集群 apiVersion:v1kind:ConfigMapmetadata:namespace:metallb-systemname:configdata:config:|address-pools: - name: default protocol: layer2 addresses: - 192.168.199.223-192.168.199.224 再查看sevice，已分配到了LoadBalancerIP: 192.168.199.223 [root@k8s-master01 ~]# k get svc istio-ingressgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.107.221.79 192.168.199.223 15021:31462/TCP,80:32243/TCP,443:31087/TCP,8081:30765/TCP 36d 配置BGP模式 开启BGP模式需要上层路由器支持BGP模式，configMap配置如下 apiVersion:v1kind:ConfigMapmetadata:namespace:metallb-systemname:configdata:config:|peers: - peer-address: 192.168.199.68 peer-asn: 64512 my-asn: 64513 address-pools: - name: default protocol: bgp addresses: - 192.168.199.223-192.168.199.224 peer-address：BGP路由器IP peer-asn: 对方自治系统编号 my-asn: 本集群自治系统编号 更多配置参考https://metallb.universe.tf/configuration ","date":"2021-08-10","objectID":"/metallb/:3:4","tags":["kubernetes","loadbalancer"],"title":"Kubernetes 裸金属环境负载均衡器 - MetalLB","uri":"/metallb/"},{"categories":["kubernetes"],"content":"模拟BGP路由器 在没有上层路由器支持BGP协议的情况下，可通过安装模拟路由器来进行测试验证，模拟路由器有BIRD、Guagga，下面以Ggugga为示例安装 # 确保yum源 yum install quagga # 关闭selinux setsebool -P zebra_write_config 1 # （可选）配置，或直接使用/etc/quagga/zebra.conf，修改其中的hostname为主机名 cp /usr/share/doc/quagga-0.99.22.4/zebra.conf.sample /etc/quagga/zebra.conf # 启动zebra systemctl start zebra # 查看zebra状态 systemctl status zebra 配置BGP cp /usr/share/doc/quagga-0.99.22.4/bgpd.conf.sample /etc/quagga/bgpd.conf # 修改/etc/quagga/bgpd.conf vi /etc/quagga/bgpd.conf ------ router bgp 64512 # 启动 systemctl start bgpd # 查看bgpd状态 systemctl status bgpd [root@node68 quagga]# vtysh Hello, this is Quagga (version 0.99.22.4). Copyright 1996-2005 Kunihiro Ishiguro, et al. node68# conf t node68(config)# router bgp 64512 node68(config-router)# no auto-summary node68(config-router)# no synchronization node68(config-router)# neighbor 192.168.199.77 remote-as 64513 node68(config-router)# neighbor 192.168.199.77 description \"k8s master01\" node68(config-router)# exit node68(config)# exit node68# write #查看BGP建立状态 node68# show ip bgp summary BGP router identifier 192.168.199.68, local AS number 64512 RIB entries 3, using 336 bytes of memory Peers 1, using 4560 bytes of memory Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 192.168.199.77 4 64513 2 3 0 0 0 00:00:17 2 Total number of neighbors 1 node68# exit # 查看主机路由信息，看到192.168.199.223和192.168.199.224的路由信息已经添加 [root@node68 ~]# ip route default via 192.168.199.1 dev eth0 169.254.0.0/16 dev eth0 scope link metric 1002 192.168.199.0/24 dev eth0 proto kernel scope link src 192.168.199.68 192.168.199.223 via 192.168.199.77 dev eth0 proto zebra 192.168.199.224 via 192.168.199.77 dev eth0 proto zebra # 通过LB ip访问集群 [root@node68 ~]# curl -v 192.168.199.223:80 {\"timestamp\":\"2021-08-13T06:40:22.413+0000\",\"status\":404,\"error\":\"Not Found\",\"message\":\"No message available\",\"path\":\"/\"} ","date":"2021-08-10","objectID":"/metallb/:3:5","tags":["kubernetes","loadbalancer"],"title":"Kubernetes 裸金属环境负载均衡器 - MetalLB","uri":"/metallb/"},{"categories":["kubernetes"],"content":"原理 Metallb包含两个组件，Controller和Speaker，Controller为Deployment部署方式，而Speaker则采用daemonset方式部署到Kubernetes集群各个Node节点。 具体的工作原理如下图所示，Controller负责监听service变化，当service配置为LoadBalancer模式时，从IP池分配给到相应的IP，并进行IP的生命周期管理。Speaker则依据Service的变化，按具体的协议发起相应的广播或应答，根据工作模式（Layer2/BGP)的不同，可采用Leader的方式或负载均衡的方式来响应请求。 当业务流量通过TCP/UDP协议到达指定的Node时，由Node上面运行的Kube-Proxy组件对流量进行处理，并分发到对应的Pod上面。 ","date":"2021-08-10","objectID":"/metallb/:4:0","tags":["kubernetes","loadbalancer"],"title":"Kubernetes 裸金属环境负载均衡器 - MetalLB","uri":"/metallb/"},{"categories":["kubernetes"],"content":"Layer 2模式 第2层模式下，Metallb会在Node节点中选出一台做为Leader，与服务IP相关的所有流量都会流向该节点。在该节点上， kube-proxy将流量传播到所有服务的Pod，而当leader节点出现故障时，会由另一个节点接管。 局限性： 在二层模式中会存在以下两种局限性：单节点瓶颈以及故障转移慢的情况。 单个leader选举节点接收服务IP的所有流量。这意味着服务的入口带宽被限制为单个节点的带宽，单节点的流量处理能力将成为整个集群的接收外部流量的瓶颈。 在当前的实现中，节点之间的故障转移取决于客户端的合作。当发生故障转移时，MetalLB发送许多2层数据包，以通知客户端与服务IP关联的MAC地址已更改。大多数操作系统能正确处理数据包，并迅速更新其邻居缓存。在这种情况下，故障转移将在几秒钟内发生。在计划外的故障转移期间，在有故障的客户端刷新其缓存条目之前，将无法访问服务IP。对于生产环境如果要求毫秒性的故障切换，目前Metallb可能会比较难适应要求。 ","date":"2021-08-10","objectID":"/metallb/:4:1","tags":["kubernetes","loadbalancer"],"title":"Kubernetes 裸金属环境负载均衡器 - MetalLB","uri":"/metallb/"},{"categories":["kubernetes"],"content":"BGP模式 在BGP模式下，群集中的每个节点都与网络路由器建立BGP对等会话，并使用该对等会话通告外部群集服务的IP。假设您的路由器配置为支持多路径，则可以实现真正的负载平衡：MetalLB发布的路由彼此等效。这意味着路由器将一起使用所有下一跳，并在它们之间进行负载平衡。数据包到达节点后，kube-proxy负责流量路由的最后一跳，将数据包送达服务中的一个特定容器。 负载平衡的方式取决于您特定的路由器型号和配置，但是常见的行为是基于数据包哈希值来平衡每个连接，这意味着单个TCP或UDP会话的所有数据包都将定向到群集中的单个计算机。 局限性： 基于BGP的路由器实现无状态负载平衡。他们通过对数据包头中的某些字段进行哈希处理，并将该哈希值用作可用后端数组的索引，将给定的数据包分配给特定的下一跳。 但路由器中使用的哈希通常不稳定，因此，只要后端集的大小发生变化（例如，当节点的BGP会话断开时），现有连接就会被随机有效地重新哈希，这意味着大多数现有连接最终将突然转发到另一后端，而该后端不知道所讨论的连接。 ","date":"2021-08-10","objectID":"/metallb/:4:2","tags":["kubernetes","loadbalancer"],"title":"Kubernetes 裸金属环境负载均衡器 - MetalLB","uri":"/metallb/"},{"categories":["kubernetes"],"content":"参考 https://metallb.universe.tf/ https://blog.csdn.net/keith6785753/article/details/107088632/ https://blog.csdn.net/kadwf123/article/details/96838805 ","date":"2021-08-10","objectID":"/metallb/:5:0","tags":["kubernetes","loadbalancer"],"title":"Kubernetes 裸金属环境负载均衡器 - MetalLB","uri":"/metallb/"}]